{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "823c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import raster_relight as rr\n",
    "import raster_dataloader as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad170497",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52eb7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LightMPL(torchvision.ops.MLP):\n",
    "    def __init__(self, num_feats, hidden_channels, norm_layer=None, activation_layer=torch.nn.modules.activation.ReLU, inplace=None, bias=True, dropout=0.0):\n",
    "        super().__init__(num_feats * 3, hidden_channels + [3], norm_layer, activation_layer, inplace, bias, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f65ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloader\n",
    "def get_dataloader(batch_size, split='train'):\n",
    "    is_train = split == 'train'\n",
    "    \"Get a dataloader for training or testing\"\n",
    "    full_dataset = rd.RasterDataset() # uses a config to decide what to load.\n",
    "    loader = torch.utils.data.DataLoader(dataset=full_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True if is_train else False, \n",
    "                                         pin_memory=True, num_workers=0)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for step, (feats, target_vector) in enumerate(valid_dl):\n",
    "            feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = torch.flatten(feats, start_dim=1)\n",
    "            outputs = model(inputs) # should be (batch_len, 3)\n",
    "            val_loss += loss_func(outputs, target_vector)*target_vector.size(0)\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            # TODO: Implement image reconstruction using validation image and model\n",
    "            # if i==batch_idx and log_images:\n",
    "            #     log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "500003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uowouo7o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▆▃▄▃▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▁▁▂▁▂▁▂▂▁▂▁▂▁▂▁▁▂▁▁▂</td></tr><tr><td>val/val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/train_loss</td><td>0.08299</td></tr><tr><td>val/val_loss</td><td>0.08236</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-hill-10</strong> at: <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/uowouo7o' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/uowouo7o</a><br/> View job at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v2' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230909_163620-uowouo7o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uowouo7o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b04ce6ebbc4f9084553605ec3fe920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112914588900619, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dtetruash/Thesis/intrinsic-light-estimation/light_mlp/wandb/run-20230909_163839-4jpnttzk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/4jpnttzk' target=\"_blank\">fresh-shadow-11</a></strong> to <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/4jpnttzk' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/4jpnttzk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from intrinsic_tester_sphere/train: 1it [00:00,  2.49it/s]\n",
      "Loading dataset from intrinsic_tester_sphere/train: 1it [00:00,  2.65it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0265719447f44ae4bd69b6c14baf5e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/2315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.093 Valid Loss: 0.088915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/2315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.079 Valid Loss: 0.083239\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>▇█▅▂▂▃▂▂▃▂▂▁▃▂▁▃▁▂▂▁▂▂▂▁▁▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/train_loss</td><td>0.07913</td></tr><tr><td>val/val_loss</td><td>0.08324</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-shadow-11</strong> at: <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/4jpnttzk' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/4jpnttzk</a><br/> View job at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v2' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230909_163839-4jpnttzk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch 5 experiments, trying different dropout rates\n",
    "\n",
    "# 🐝 initialise a wandb run\n",
    "# NOTE: The model checkpoint path should be to scratch on the cluster\n",
    "\n",
    "current_run = wandb.init(\n",
    "    project=\"light-mlp-supervised-cosine\",\n",
    "    config={\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 1024,\n",
    "        \"lr\": 1e-3,\n",
    "        \"dropout\": 0.0, # random.uniform(0.01, 0.80),\n",
    "        \"num_feats\": 3,\n",
    "        \"hidden_channels\" : [256]*4 + [128], \n",
    "        \"model_checkpoint_path\" : 'model_checkpoints',\n",
    "        'model_trained_path' : 'model_trained',\n",
    "        })\n",
    "\n",
    "# Copy your config \n",
    "config = wandb.config\n",
    "\n",
    "# make output dirs\n",
    "if not os.path.exists(config.model_checkpoint_path):\n",
    "    os.makedirs(config.model_checkpoint_path)\n",
    "\n",
    "if not os.path.exists(config.model_trained_path):\n",
    "    os.makedirs(config.model_trained_path)\n",
    "\n",
    "# Get the data\n",
    "train_dl = get_dataloader(batch_size=config.batch_size)\n",
    "valid_dl = get_dataloader(batch_size=2*config.batch_size, split='valid')\n",
    "\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "# MLP model\n",
    "model = LightMPL(config.num_feats, config.hidden_channels, dropout=config.dropout)\n",
    "\n",
    "# Make the loss and optimizer\n",
    "cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#cosine_target = torch.tensor([1.0]).to(device)\n",
    "def loss_func(x,y):\n",
    "    cosine_similarity = cosine(x,y)\n",
    "    similarity_target = torch.tensor([1.0]).broadcast_to(cosine_similarity.size()).to(device)\n",
    "    return F.mse_loss(cosine_similarity, similarity_target)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Training\n",
    "step_ct = 0\n",
    "for epoch in tqdm(range(config.epochs),\n",
    "                  desc=\"Epoch\", \n",
    "                  total=config.epochs, \n",
    "                  position=0, leave=True, colour='green'):\n",
    "    model.train()\n",
    "    for step, (feats, target_vector) in tqdm(enumerate(train_dl), \n",
    "                                       total=len(train_dl), \n",
    "                                       desc=f\"Step\",\n",
    "                                       position=1, leave=False, colour='red'):\n",
    "        # Move to device\n",
    "        feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.flatten(feats, start_dim=1)\n",
    "        outputs = model(inputs) # should be (batch_len, 3)\n",
    "        train_loss = loss_func(outputs, target_vector)\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect metrics\n",
    "        metrics = {\"train/train_loss\": train_loss, \n",
    "                   \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                   }\n",
    "\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # 🐝 Log train metrics to wandb \n",
    "            wandb.log(metrics)\n",
    "\n",
    "        step_ct += 1\n",
    "\n",
    "    # Validation\n",
    "    val_loss = validate_model(model, valid_dl, loss_func)\n",
    "\n",
    "    # 🐝 Log train and validation metrics to wandb\n",
    "    val_metrics = {\"val/val_loss\": val_loss}\n",
    "    wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f} Valid Loss: {val_loss:3f}\")\n",
    "\n",
    "    # save the model and upload it to wandb\n",
    "    if epoch + 1 == config.epochs:\n",
    "        # save trained model\n",
    "        trained_file = f\"{config.model_trained_path}/{current_run.project}_{current_run.name}.pth\"\n",
    "        torch.save(model.state_dict(),  trained_file)\n",
    "        wandb.save(trained_file, policy='now')\n",
    "    else:\n",
    "        # save checkpoint\n",
    "        check_point_file = f\"{config.model_checkpoint_path}/{current_run.project}_{current_run.name}_ckpt.pth\"\n",
    "        torch.save(model.state_dict(), check_point_file)\n",
    "        wandb.save(check_point_file, policy='live')\n",
    "\n",
    "\n",
    "# 🐝 Close your wandb run \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101626c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
