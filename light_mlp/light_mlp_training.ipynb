{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "823c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import raster_relight as rr\n",
    "import raster_dataloader as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad170497",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52eb7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LightMPL(torchvision.ops.MLP):\n",
    "    def __init__(self, num_feats, hidden_channels, norm_layer=None, activation_layer=torch.nn.modules.activation.ReLU, inplace=None, bias=True, dropout=0.0):\n",
    "        super().__init__(num_feats * 3, hidden_channels + [3], norm_layer, activation_layer, inplace, bias, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f65ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloader\n",
    "def get_dataloader(batch_size, split='train'):\n",
    "    is_train = split == 'train'\n",
    "    \"Get a dataloader for training or testing\"\n",
    "    full_dataset = rd.RasterDataset() # uses a config to decide what to load.\n",
    "    loader = torch.utils.data.DataLoader(dataset=full_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True if is_train else False, \n",
    "                                         pin_memory=True, num_workers=0)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac47148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for step, (feats, target_vector) in enumerate(valid_dl):\n",
    "            feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = torch.flatten(feats, start_dim=1)\n",
    "            outputs = model(inputs) # should be (batch_len, 3)\n",
    "            val_loss += loss_func(outputs, target_vector)*target_vector.size(0)\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            # TODO: Implement image reconstruction using validation image and model\n",
    "            # if i==batch_idx and log_images:\n",
    "            #     log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bbd54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from intrinsic_tester_sphere/: 1it [00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dl: feats have shape of torch.Size([2370105, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = get_dataloader(True, 1024)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "500003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51b9e4e4a7b4915b570420fa0dd79c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113601444473412, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dtetruash/Thesis/intrinsic-light-estimation/light_mlp/wandb/run-20230909_000130-1944n6o8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtetruash/light-mlp/runs/1944n6o8' target=\"_blank\">tough-music-28</a></strong> to <a href='https://wandb.ai/dtetruash/light-mlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtetruash/light-mlp' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtetruash/light-mlp/runs/1944n6o8' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp/runs/1944n6o8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from intrinsic_tester_sphere/: 1it [00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dl: feats have shape of torch.Size([2370105, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from intrinsic_tester_sphere/: 1it [00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dl: feats have shape of torch.Size([2370105, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3317fff1814affa021fd8c8b0735e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/2315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94445/154333849.py:31: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_func = lambda x,y: F.mse_loss(cosine(x,y), cosine_target)\n",
      "/tmp/ipykernel_94445/154333849.py:31: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([569])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_func = lambda x,y: F.mse_loss(cosine(x,y), cosine_target)\n",
      "/tmp/ipykernel_94445/154333849.py:31: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([2048])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_func = lambda x,y: F.mse_loss(cosine(x,y), cosine_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.076 Valid Loss: 0.087240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/2315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.087 Valid Loss: 0.082285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▄▃▃▄▂▄▂▃▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▁▁▂▁▁▂▁▂</td></tr><tr><td>val/val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/train_loss</td><td>0.08689</td></tr><tr><td>val/val_loss</td><td>0.08229</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-music-28</strong> at: <a href='https://wandb.ai/dtetruash/light-mlp/runs/1944n6o8' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp/runs/1944n6o8</a><br/> View job at <a href='https://wandb.ai/dtetruash/light-mlp/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk1OTgwNTQ0/version_details/v2' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk1OTgwNTQ0/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230909_000130-1944n6o8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch 5 experiments, trying different dropout rates\n",
    "\n",
    "# 🐝 initialise a wandb run\n",
    "wandb.init(\n",
    "    project=\"light-mlp-supervised-cosine\",\n",
    "    config={\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 1024,\n",
    "        \"lr\": 1e-3,\n",
    "        \"dropout\": 0.0, # random.uniform(0.01, 0.80),\n",
    "        \"num_feats\": 3,\n",
    "        \"hidden_channels\" : [256]*4 + [128], \n",
    "        })\n",
    "\n",
    "# Copy your config \n",
    "config = wandb.config\n",
    "\n",
    "# Get the data\n",
    "train_dl = get_dataloader(batch_size=config.batch_size)\n",
    "valid_dl = get_dataloader(batch_size=2*config.batch_size, split='valid')\n",
    "\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "# MLP model\n",
    "model = LightMPL(config.num_feats, config.hidden_channels, dropout=config.dropout)\n",
    "\n",
    "# Make the loss and optimizer\n",
    "cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "# NOTE: don't forget that this needs to be exteded to be to twice the length for validation loop\n",
    "cosine_target = torch.tensor([1.0]).to(device)\n",
    "loss_func = lambda x,y: F.mse_loss(cosine(x,y), cosine_target)\n",
    "#valid_loss_func = lambda x,y: F.mse_loss(cosine(x,y), torch.cat([cosine_target,cosine_target]))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Training\n",
    "step_ct = 0\n",
    "for epoch in tqdm(range(config.epochs),\n",
    "                  desc=\"Epoch\", \n",
    "                  total=config.epochs, \n",
    "                  position=0, leave=True, colour='green'):\n",
    "    model.train()\n",
    "    for step, (feats, target_vector) in tqdm(enumerate(train_dl), \n",
    "                                       total=len(train_dl), \n",
    "                                       desc=f\"Step\",\n",
    "                                       position=1, leave=False, colour='red'):\n",
    "        # Move to device\n",
    "        feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.flatten(feats, start_dim=1)\n",
    "        outputs = model(inputs) # should be (batch_len, 3)\n",
    "        train_loss = loss_func(outputs, target_vector)\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect metrics\n",
    "        metrics = {\"train/train_loss\": train_loss, \n",
    "                   \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                   }\n",
    "\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # 🐝 Log train metrics to wandb \n",
    "            wandb.log(metrics)\n",
    "\n",
    "        step_ct += 1\n",
    "\n",
    "    # Validation\n",
    "    val_loss = validate_model(model, valid_dl, loss_func)\n",
    "\n",
    "    # 🐝 Log train and validation metrics to wandb\n",
    "    val_metrics = {\"val/val_loss\": val_loss}\n",
    "    wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f} Valid Loss: {val_loss:3f}\")\n",
    "\n",
    "# 🐝 Close your wandb run \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a87f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
