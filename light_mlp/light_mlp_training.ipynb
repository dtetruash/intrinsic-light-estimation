{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823c63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtetruash/.miniconda3/envs/thesis/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/dtetruash/.miniconda3/envs/thesis/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import raster_relight as rr\n",
    "import raster_dataloader as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad170497",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52eb7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LightMPL(torchvision.ops.MLP):\n",
    "    def __init__(self, num_feats, hidden_channels, norm_layer=None, activation_layer=torch.nn.modules.activation.ReLU, inplace=None, bias=True, dropout=0.0):\n",
    "        super().__init__(num_feats * 3, hidden_channels + [3], norm_layer, activation_layer, inplace, bias, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = F.normalize(x)\n",
    "        assert x.shape[-1] == 3\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f65ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloader\n",
    "def get_dataloader(batch_size, split='train'):\n",
    "    is_train = split == 'train'\n",
    "    \"Get a dataloader for training or testing\"\n",
    "    full_dataset = rd.RasterDataset(split) # uses a config to decide what to load.\n",
    "    loader = torch.utils.data.DataLoader(dataset=full_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=is_train, \n",
    "                                         pin_memory=True, num_workers=0)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac47148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for step, (feats, target_vector) in tqdm(enumerate(valid_dl), \n",
    "                                                 desc=\"Validating model\", \n",
    "                                                 total = len(valid_dl)):\n",
    "            feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = torch.flatten(feats, start_dim=1)\n",
    "            outputs = model(inputs) # should be (batch_len, 3)\n",
    "            val_loss += loss_func(outputs, target_vector)*target_vector.size(0)\n",
    "    \n",
    "    return val_loss / len(valid_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75527254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_image(model, valid_dataset):\n",
    "    \"\"\"Generate an image comparing a ground truth image with one generated using the model.\n",
    "    model: MLP which outputs light direction vectors\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        # Randomly choose which image from the validation set to reconstruct\n",
    "        image_number = np.random.randint(valid_dataset.num_frames)\n",
    "        # randomly choose a light in the scene\n",
    "        light_names = list(valid_dataset._lights_info.keys())\n",
    "        light_name = light_names[np.random.randint(valid_dataset._num_lights)]\n",
    "        print(f\"Generating OLAT validation image {image_number} with light {light_name}...\")\n",
    "        \n",
    "        # load attributes of this validation image\n",
    "        W, H, raster_image_pixels, world_normals, albedo, occupancy_mask = valid_dataset.attributes[image_number][light_name]\n",
    "        raster_image_pixels = raster_image_pixels.astype(np.float32)\n",
    "        world_normals = world_normals.astype(np.float32)\n",
    "        albedo = albedo.astype(np.float32)\n",
    "        # print(f\"VALIMG: Albedo had shape {albedo.shape} and type {albedo.dtype}\")\n",
    "        # print(f\"VALIMG: world_normals had shape {world_normals.shape} and type {world_normals.dtype}\")\n",
    "        # print(f\"VALIMG: raster_image_pixels had shape {raster_image_pixels.shape} and type {raster_image_pixels.dtype}\")\n",
    "        # print(f\"VALIMG: occupancy_mask had shape {occupancy_mask.shape} and type {occupancy_mask.dtype}\")\n",
    "        \n",
    "        \n",
    "        # prepare inputs for inference\n",
    "        feats = np.stack([world_normals, albedo, raster_image_pixels], axis=1)\n",
    "        inputs = torch.flatten(torch.as_tensor(feats).float(), start_dim=1)\n",
    "        # print(f\"VALIMG: inputs had shape {inputs.shape} and type {inputs.dtype}\")\n",
    "        \n",
    "        # Do inference to get light vectors\n",
    "        light_vectors = model(inputs)\n",
    "        light_vectors = light_vectors.numpy().astype(np.float32)\n",
    "        print(f\"inf vectors were: {light_vectors}\")\n",
    "        # print(f\"VALIMG: Inf Light vecs have shape {light_vectors.shape} and type {light_vectors.dtype}\")\n",
    "        \n",
    "        # Construct a normals image\n",
    "        img_size =(W,H,3)\n",
    "        val_norm_image = np.ones(img_size)\n",
    "        val_norms = 0.5*light_vectors + 0.5\n",
    "        val_norm_image[occupancy_mask] = val_norms\n",
    "        \n",
    "        # Construct validation image\n",
    "        val_image = np.ones(img_size)\n",
    "        val_raster_pixels =  rr.raster_from_directions(light_vectors, albedo, world_normals)\n",
    "        # print(f\"VALIMG: val_raster_pixels have shape {val_raster_pixels.shape} and type {val_raster_pixels.dtype}\")\n",
    "        \n",
    "        val_image[occupancy_mask] = val_raster_pixels\n",
    "        \n",
    "        # Construct GT image\n",
    "        gt_image = np.ones(img_size)\n",
    "        gt_image[occupancy_mask] = raster_image_pixels\n",
    "        \n",
    "    return np.concatenate([val_norm_image, val_image, gt_image], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_dl, model, loss_func):\n",
    "    cumu_loss = 0.0\n",
    "    for step, (feats, target_vector) in tqdm(enumerate(train_dl), \n",
    "                                       total=len(train_dl), \n",
    "                                       desc=f\"Step\",\n",
    "                                       position=1, leave=False, colour='red'):\n",
    "        # Move to device\n",
    "        feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "        #print(f\"feats size: {feats.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.flatten(feats, start_dim=1) # (batch_len, )\n",
    "        outputs = model(inputs) # should be (batch_len, 3)\n",
    "        train_loss = loss_func(outputs, target_vector)\n",
    "        cumu_loss += train_loss.item()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect metrics\n",
    "        metrics = {\"train/train_loss\": train_loss, \n",
    "                   \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                   }\n",
    "\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # üêù Log train metrics to wandb \n",
    "            wandb.log(metrics)\n",
    "        \n",
    "    return cumu_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab3070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the loss and optimizer\n",
    "cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#cosine_target = torch.tensor([1.0]).to(device)\n",
    "def loss_func(x,y, lamb=2.0):\n",
    "    cosine_similarity = cosine(x,y)\n",
    "    similarity_target = torch.tensor([1.0]).broadcast_to(cosine_similarity.size()).to(device)\n",
    "    similarity_term = F.mse_loss(cosine_similarity, similarity_target)\n",
    "    \n",
    "    x_norms = torch.linalg.norm(x, dim=-1)\n",
    "    unitarity_term = F.mse_loss(x_norms, \n",
    "                                torch.tensor([1.0]).broadcast_to(x_norms.size()).to(device))\n",
    "    return similarity_term + lamb * unitarity_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c25e4288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from chair_intrinsic/train: 100%|‚ñà| 140/140 [00:07<00:00, 19.77it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from chair_intrinsic/val: 100%|‚ñà| 140/140 [00:07<00:00, 19.70it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation dataset.\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "batch_size = 1024\n",
    "train_dl = get_dataloader(batch_size=batch_size)\n",
    "print(\"Loaded train dataset.\")\n",
    "\n",
    "valid_dl = get_dataloader(batch_size=2*batch_size, split='val')\n",
    "print(\"Loaded validation dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "797001df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:m2tuxsbt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc96e5b039a42e08caa86b90fd48a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-cosmos-47</strong> at: <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/m2tuxsbt' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/m2tuxsbt</a><br/> View job at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v9' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230911_161147-m2tuxsbt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:m2tuxsbt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d4b982f69d4ee08009cfa930134619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112789055510398, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dtetruash/Thesis/intrinsic-light-estimation/light_mlp/wandb/run-20230911_161946-0kve3zaj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/0kve3zaj' target=\"_blank\">breezy-plasma-48</a></strong> to <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/0kve3zaj' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/0kve3zaj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üêù initialise a wandb run\n",
    "# NOTE: The model checkpoint path should be to scratch on the cluster\n",
    "\n",
    "raster_config = rr.parse_config()\n",
    "\n",
    "current_run = wandb.init(\n",
    "    project=\"light-mlp-supervised-cosine\",\n",
    "    config={\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": 1e-3,\n",
    "        \"dropout\": 0.0, # random.uniform(0.01, 0.80),\n",
    "        \"num_feats\": 3,\n",
    "        \"hidden_channels\" : [256]*4 + [128], \n",
    "        \"model_checkpoint_path\" : 'model_checkpoints',\n",
    "        'model_trained_path' : 'model_trained',\n",
    "        'scene' : raster_config['paths']['scene']\n",
    "        })\n",
    "\n",
    "# Copy your config \n",
    "config = wandb.config\n",
    "\n",
    "# make output dirs\n",
    "if not os.path.exists(config.model_checkpoint_path):\n",
    "    os.makedirs(config.model_checkpoint_path)\n",
    "\n",
    "if not os.path.exists(config.model_trained_path):\n",
    "    os.makedirs(config.model_trained_path)\n",
    "\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "model = LightMPL(config.num_feats, config.hidden_channels, dropout=config.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "875e49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e1c5d539f84180af6fba646f516dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974a881a35e7425a8cc764747f36b0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validating.\n",
      "Creating validation image.\n",
      "Generating OLAT validation image 89 with light arealightup...\n",
      "inf vectors were: [[-0.34993544 -0.02144917  0.9365282 ]\n",
      " [-0.35065106 -0.0092955   0.9364601 ]\n",
      " [-0.35131702  0.07042827  0.9336039 ]\n",
      " ...\n",
      " [-0.35270765 -0.01188879  0.93565804]\n",
      " [-0.34588248  0.29202062  0.89167786]\n",
      " [-0.33980444  0.33004725  0.8806826 ]]\n",
      "Train Loss: 0.016 Valid Loss: 0.013527\n",
      "Training epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ec741c53894ea69326ad745ce4167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validating.\n",
      "Creating validation image.\n",
      "Generating OLAT validation image 54 with light arealightup...\n",
      "inf vectors were: [[-0.34974736  0.22969356  0.90824974]\n",
      " [-0.34942618  0.23124664  0.90797925]\n",
      " [-0.34900504  0.2402474   0.90580165]\n",
      " ...\n",
      " [-0.35244212  0.2161377   0.9105323 ]\n",
      " [-0.3432146   0.35053292  0.87139565]\n",
      " [-0.34348428  0.32033092  0.8828401 ]]\n",
      "Train Loss: 0.013 Valid Loss: 0.012518\n",
      "Training epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84257b68aad24765a28d1479530fdc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validating.\n",
      "Creating validation image.\n",
      "Generating OLAT validation image 56 with light arealightup...\n",
      "inf vectors were: [[-0.31687444 -0.36200446  0.876666  ]\n",
      " [-0.32871112 -0.2922348   0.8980801 ]\n",
      " [-0.34276137 -0.15007924  0.92735696]\n",
      " ...\n",
      " [-0.3536987   0.1276776   0.9266044 ]\n",
      " [-0.3495583   0.24820447  0.9034399 ]\n",
      " [-0.3477556   0.32176402  0.880644  ]]\n",
      "Train Loss: 0.013 Valid Loss: 0.012350\n",
      "Training epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0a4455e6ff46e0985bd7c46bc6552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validating.\n",
      "Creating validation image.\n",
      "Generating OLAT validation image 37 with light arealightup...\n",
      "inf vectors were: [[-0.29519707 -0.48159972  0.825179  ]\n",
      " [-0.3333502  -0.25963497  0.90634835]\n",
      " [-0.333368   -0.2595531   0.9063652 ]\n",
      " ...\n",
      " [-0.33084112 -0.2756182   0.9025402 ]\n",
      " [-0.33129346 -0.27182192  0.90352505]\n",
      " [-0.32665643 -0.3049314   0.89460176]]\n",
      "Train Loss: 0.012 Valid Loss: 0.012238\n",
      "Training epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02358b22a65f48c6a21d0a2e5f86d8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validating.\n",
      "Creating validation image.\n",
      "Generating OLAT validation image 45 with light arealightfront...\n",
      "inf vectors were: [[-0.3247084  -0.31345576  0.89236194]\n",
      " [-0.27555445 -0.5539451   0.78563   ]\n",
      " [-0.28484517 -0.5171006   0.8071371 ]\n",
      " ...\n",
      " [-0.30988857 -0.39521453  0.86473966]\n",
      " [-0.33409595 -0.24141121  0.91109854]\n",
      " [-0.33602688 -0.22501172  0.91457945]]\n",
      "Train Loss: 0.012 Valid Loss: 0.012173\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/avg_loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/train_loss</td><td>‚ñà‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ</td></tr><tr><td>val/val_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/avg_loss</td><td>0.01216</td></tr><tr><td>train/epoch</td><td>4.99989</td></tr><tr><td>train/train_loss</td><td>0.01209</td></tr><tr><td>val/val_loss</td><td>0.01217</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-plasma-48</strong> at: <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/0kve3zaj' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/0kve3zaj</a><br/> View job at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v10' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTYyMzQ1/version_details/v10</a><br/>Synced 6 W&B file(s), 5 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230911_161946-0kve3zaj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in tqdm(range(config.epochs),\n",
    "                  desc=\"Epoch\", \n",
    "                  total=config.epochs, \n",
    "                  position=0, leave=True, colour='green'):\n",
    "    model.train()\n",
    "    \n",
    "    # Train an epoch\n",
    "    print(f\"Training epoch {epoch}\")\n",
    "    avg_train_loss = train_epoch(epoch, train_dl, model, loss_func)\n",
    "    wandb.log({\"train/avg_loss\": avg_train_loss})\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Validating model after epoch {epoch}\")\n",
    "    val_loss = validate_model(model, valid_dl, loss_func)\n",
    "    print(\"Done validating.\")\n",
    "    \n",
    "    # Render a validation image\n",
    "    print(\"Creating validation image.\")\n",
    "    val_image_array = generate_validation_image(model, valid_dl.dataset)\n",
    "    #val_image = PILImage.fromarray(val_image_array, mode=\"RGB\")\n",
    "    val_image = wandb.Image(val_image_array, caption='Top: Infered Light, Bottom: GT')\n",
    "    #print(f\"image type {type(val_image)}\")\n",
    "    #val_image.save(f\"validation_image_{epoch:03}.png\")\n",
    "\n",
    "    # üêù Log train and validation metrics to wandb\n",
    "    val_metrics = {\"val/val_loss\": val_loss,\n",
    "                   \"val/images\": val_image}\n",
    "    wandb.log(val_metrics)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.3f} Valid Loss: {val_loss:3f}\")\n",
    "\n",
    "    # save the model and upload it to wandb\n",
    "    if epoch + 1 == config.epochs:\n",
    "        # save trained model\n",
    "        trained_file = f\"{config.model_trained_path}/{current_run.project}_{current_run.name}.pth\"\n",
    "        torch.save(model.state_dict(),  trained_file)\n",
    "        wandb.save(trained_file, policy='now')\n",
    "    else:\n",
    "        # save checkpoint\n",
    "        check_point_file = f\"{config.model_checkpoint_path}/{current_run.project}_{current_run.name}_ckpt.pth\"\n",
    "        torch.save(model.state_dict(), check_point_file)\n",
    "        wandb.save(check_point_file, policy='live')\n",
    "\n",
    "\n",
    "# üêù Close your wandb run \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc09a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff8011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88788e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
