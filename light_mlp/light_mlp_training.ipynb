{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823c63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtetruash/.miniconda3/envs/thesis/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/dtetruash/.miniconda3/envs/thesis/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import raster_relight as rr\n",
    "import raster_dataloader as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad170497",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52eb7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LightMPL(torchvision.ops.MLP):\n",
    "    def __init__(self, num_feats, hidden_channels, norm_layer=None, activation_layer=torch.nn.modules.activation.ReLU, inplace=None, bias=True, dropout=0.0):\n",
    "        super().__init__(num_feats * 3, hidden_channels + [3], norm_layer, activation_layer, inplace, bias, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f65ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloader\n",
    "def get_dataloader(batch_size, split='train'):\n",
    "    is_train = split == 'train'\n",
    "    \"Get a dataloader for training or testing\"\n",
    "    full_dataset = rd.RasterDataset(split) # uses a config to decide what to load.\n",
    "    loader = torch.utils.data.DataLoader(dataset=full_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=is_train, \n",
    "                                         pin_memory=True, num_workers=0)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac47148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for step, (feats, target_vector) in tqdm(enumerate(valid_dl), \n",
    "                                                 desc=\"Validating model\", \n",
    "                                                 total = len(valid_dl)):\n",
    "            feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = torch.flatten(feats, start_dim=1)\n",
    "            outputs = model(inputs) # should be (batch_len, 3)\n",
    "            val_loss += loss_func(outputs, target_vector)*target_vector.size(0)\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            # TODO: Implement image reconstruction using validation image and model\n",
    "            # if i==batch_idx and log_images:\n",
    "            #     log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75527254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_image(model, valid_dataset):\n",
    "    \"\"\"Generate an image comparing a ground truth image with one generated using the model.\n",
    "    model: MLP which outputs light direction vectors\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        # Randomly choose which image from the validation set to reconstruct\n",
    "        image_number = np.random.randint(valid_dataset.num_frames)\n",
    "        # randomly choose a light in the scene\n",
    "        light_names = list(valid_dataset._lights_info.keys())\n",
    "        light_name = light_names[np.random.randint(valid_dataset._num_lights)]\n",
    "        print(f\"Generating OLAT validation image {image_number} with light {light_name}...\")\n",
    "        \n",
    "        # load attributes of this validation image\n",
    "        W, H, raster_image_pixels, world_normals, albedo, occupancy_mask = valid_dataset.attributes[image_number][light_name]\n",
    "        raster_image_pixels = raster_image_pixels.astype(np.float32)\n",
    "        world_normals = world_normals.astype(np.float32)\n",
    "        albedo = albedo.astype(np.float32)\n",
    "        print(f\"VALIMG: Albedo had shape {albedo.shape} and type {albedo.dtype}\")\n",
    "        print(f\"VALIMG: world_normals had shape {world_normals.shape} and type {world_normals.dtype}\")\n",
    "        print(f\"VALIMG: raster_image_pixels had shape {raster_image_pixels.shape} and type {raster_image_pixels.dtype}\")\n",
    "        print(f\"VALIMG: occupancy_mask had shape {occupancy_mask.shape} and type {occupancy_mask.dtype}\")\n",
    "        \n",
    "        \n",
    "        # prepare inputs for inference\n",
    "        feats = np.stack([world_normals, albedo, raster_image_pixels], axis=1)\n",
    "        inputs = torch.flatten(torch.as_tensor(feats).float(), start_dim=1)\n",
    "        print(f\"VALIMG: inputs had shape {inputs.shape} and type {inputs.dtype}\")\n",
    "        \n",
    "        # Do inference to get light vectors\n",
    "        light_vectors = model(inputs)\n",
    "        light_vectors = light_vectors.numpy().astype(np.float32)\n",
    "        print(f\"VALIMG: Inf Light vecs have shape {light_vectors.shape} and type {light_vectors.dtype}\")\n",
    "        \n",
    "        \n",
    "        # Construct validation image\n",
    "        val_image = np.ones((W,H,3))\n",
    "        val_raster_pixels =  rr.raster_from_directions(light_vectors, albedo, world_normals)\n",
    "        print(f\"VALIMG: val_raster_pixels have shape {val_raster_pixels.shape} and type {val_raster_pixels.dtype}\")\n",
    "        \n",
    "        val_image[occupancy_mask] = val_raster_pixels\n",
    "        \n",
    "        # Construct GT image\n",
    "        gt_image = np.ones((W,H,3))\n",
    "        gt_image[occupancy_mask] = raster_image_pixels\n",
    "        \n",
    "    return np.concatenate([val_image, gt_image], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_dl, model, loss_func):\n",
    "    cumu_loss = 0.0\n",
    "    for step, (feats, target_vector) in tqdm(enumerate(train_dl), \n",
    "                                       total=len(train_dl), \n",
    "                                       desc=f\"Step\",\n",
    "                                       position=1, leave=False, colour='red'):\n",
    "        # Move to device\n",
    "        feats, target_vector = feats.to(device), target_vector.to(device)\n",
    "        #print(f\"feats size: {feats.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.flatten(feats, start_dim=1) # (batch_len, )\n",
    "        outputs = model(inputs) # should be (batch_len, 3)\n",
    "        train_loss = loss_func(outputs, target_vector)\n",
    "        cumu_loss += train_loss.item()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect metrics\n",
    "        metrics = {\"train/train_loss\": train_loss, \n",
    "                   \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                   }\n",
    "\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # ðŸ Log train metrics to wandb \n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        break\n",
    "        \n",
    "    return cumu_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875e49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdtetruash\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dtetruash/Thesis/intrinsic-light-estimation/light_mlp/wandb/run-20230910_193826-l93amdy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/l93amdy5' target=\"_blank\">celestial-pond-28</a></strong> to <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/l93amdy5' target=\"_blank\">https://wandb.ai/dtetruash/light-mlp-supervised-cosine/runs/l93amdy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from chair_intrinsic/train: 100%|â–ˆ| 140/140 [00:08<00:00, 17.45it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from chair_intrinsic/val: 100%|â–ˆ| 140/140 [00:07<00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation dataset.\n"
     ]
    }
   ],
   "source": [
    "# Launch 5 experiments, trying different dropout rates\n",
    "\n",
    "# ðŸ initialise a wandb run\n",
    "# NOTE: The model checkpoint path should be to scratch on the cluster\n",
    "\n",
    "current_run = wandb.init(\n",
    "    project=\"light-mlp-supervised-cosine\",\n",
    "    config={\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 1024,\n",
    "        \"lr\": 1e-3,\n",
    "        \"dropout\": 0.0, # random.uniform(0.01, 0.80),\n",
    "        \"num_feats\": 3,\n",
    "        \"hidden_channels\" : [256]*4 + [128], \n",
    "        \"model_checkpoint_path\" : 'model_checkpoints',\n",
    "        'model_trained_path' : 'model_trained',\n",
    "        })\n",
    "\n",
    "# Copy your config \n",
    "config = wandb.config\n",
    "\n",
    "# make output dirs\n",
    "if not os.path.exists(config.model_checkpoint_path):\n",
    "    os.makedirs(config.model_checkpoint_path)\n",
    "\n",
    "if not os.path.exists(config.model_trained_path):\n",
    "    os.makedirs(config.model_trained_path)\n",
    "\n",
    "# Get the data\n",
    "train_dl = get_dataloader(batch_size=config.batch_size)\n",
    "print(\"Loaded train dataset.\")\n",
    "\n",
    "valid_dl = get_dataloader(batch_size=2*config.batch_size, split='val')\n",
    "print(\"Loaded validation dataset.\")\n",
    "\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "# MLP model\n",
    "model = LightMPL(config.num_feats, config.hidden_channels, dropout=config.dropout)\n",
    "\n",
    "# Make the loss and optimizer\n",
    "cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#cosine_target = torch.tensor([1.0]).to(device)\n",
    "def loss_func(x,y):\n",
    "    cosine_similarity = cosine(x,y)\n",
    "    similarity_target = torch.tensor([1.0]).broadcast_to(cosine_similarity.size()).to(device)\n",
    "    return F.mse_loss(cosine_similarity, similarity_target)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a14fd45e8c64183a5d82c39ab72e459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e72424408b4de48e372f665356602b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/9442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model after epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302bfcba85b64b789ec336b8f0663122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating model:   0%|          | 0/4721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in tqdm(range(config.epochs),\n",
    "                  desc=\"Epoch\", \n",
    "                  total=config.epochs, \n",
    "                  position=0, leave=True, colour='green'):\n",
    "    model.train()\n",
    "    \n",
    "    # Train an epoch\n",
    "    print(f\"Training epoch {epoch}\")\n",
    "    avg_loss = train_epoch(epoch, train_dl, model, loss_func)\n",
    "    wandb.log({\"train/avg_loss\": avg_loss})\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Validating model after epoch {epoch}\")\n",
    "    val_loss = validate_model(model, valid_dl, loss_func)\n",
    "    print(\"Done validating.\")\n",
    "    \n",
    "    # Render a validation image\n",
    "    print(\"Creating validation image.\")\n",
    "    val_image_array = generate_validation_image(model, valid_dl.dataset)\n",
    "    val_image = wandb.Image(val_image, caption='Top: Infered Light, Bottom: GT')\n",
    "    val_image.save(f\"validation_image_{epoch:03}.png\")\n",
    "\n",
    "    # ðŸ Log train and validation metrics to wandb\n",
    "    val_metrics = {\"val/val_loss\": val_loss,\n",
    "                   \"val/images\": val_image}\n",
    "    wandb.log(val_metrics)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f} Valid Loss: {val_loss:3f}\")\n",
    "\n",
    "    # save the model and upload it to wandb\n",
    "    if epoch + 1 == config.epochs:\n",
    "        # save trained model\n",
    "        trained_file = f\"{config.model_trained_path}/{current_run.project}_{current_run.name}.pth\"\n",
    "        torch.save(model.state_dict(),  trained_file)\n",
    "        wandb.save(trained_file, policy='now')\n",
    "    else:\n",
    "        # save checkpoint\n",
    "        check_point_file = f\"{config.model_checkpoint_path}/{current_run.project}_{current_run.name}_ckpt.pth\"\n",
    "        torch.save(model.state_dict(), check_point_file)\n",
    "        wandb.save(check_point_file, policy='live')\n",
    "\n",
    "\n",
    "# ðŸ Close your wandb run \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc09a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff8011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88788e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
